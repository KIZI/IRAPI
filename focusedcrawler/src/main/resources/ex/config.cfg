# $Id: config.cfg.template 885 2007-02-27 21:47:18Z labsky $
# default logger settings
log_file = ex.log
log_level = WRN
# WRN, FPI:TRC
log_level_ = TRC, Document:TRC, ACFinder:WRN, ac:TRC, HtmlDomParser:TRC, HtmlSaxParser:WRN, Parser:WRN, dsw:INF, PhraseSource:WRN, ConsoleClient:ERR, crf:INF \ 
	    Book:INF, FPI:TRC, Reader:INF, Model:TRC, ModelReader:TRC, FM:TRC, AC:INF, Parser:TRC, ICLattice:TRC, ConsoleClient:ERR, \
            PatComp:TRC, PatMatcher:TRC, ICTrie:INF, IC:TRC, SYS:WRN, Trie:WRN, Document:TRC, \
            DbLoader:TRC, Downloader:TRC, Fetcher:TRC
# log_cp = windows-1250
log_cp = utf-8
log_append = 0
log_time = 1
enable_pause = 0

# parser
parser_nbest = 1
parser_nbest_show = 1
# generate instances & standalone attributes or just standalone attributes
parser_gen_inst = 1
# how many dom levels from the bottom should we search for attribute candidates belonging to the same instance
parser_max_dom_climb = -1
# how many elements can be at most included as orphans into an instance candidate
parser_max_skip = 0
# cost of all orphans times this factor is compared to the cost of IC's member attributes, if it is higher, IC is discarded
parser_orphan_penalty_factor = 0.2
# whether to allow standalone attributes inside instances
parser_mixed_mode = 0
# beam settings for IC generation: for each token, the following absolute and relative limits can apply:
# only the ICs that have confidence at 40% or better than the most probable IC at every member token are kept
parser_beam_width_rel=0.9
# at most the following number of most confident ICs is let to live at every member token
parser_beam_width_abs=3
# minimal AC probability required (0.01 default)
ac_prune_threshold = 0.49
# add this small constant to the ac probability for each its token
ac_length_boost = 1e-7
# when to stop ACSegment generation: factor expresses ratio of the probability of the shortest ACSegment generated to the longer ones
acsegment_prune_factor = 120
# whether to include partial ACs in ACSegments
acsegment_inc_part_acs = 1
# induce formatting patterns within Document/DocumentSet
induce_fmt_patterns = 0
# factors to be applied to count-based induced fmt pattern precision and recall 
fmtpat_pfactor = 1
fmtpat_rfactor = 0.5
# pruning parameters
max_ic_cnt = 5000
max_parse_time = 30000
# use normalization factor in order not to damage AC orphan & engaged probs vs. mistake prob
normalize_ac_probs = 1

# debug: for each AC, shows all patterns involved in AC creation
ac_show_patterns = 0
# debug: shows complete path through document including background states; otherwise only instances and standalone attributes are shown
parser_show_bg = 0

# default evidence indicating an AC presence includes: non-crossed inline tags, non-crossed block tags, whole AC fits exactly into parent tag
# 0: default evidence ignored, 1: only used when it does not hold, 2: used always (as other evidence)
default_evidence_mode = 1

# length and numeric value evidence modes
length_evidence_mode = 2
numval_evidence_mode = 2

# text processing
tokenizer=uep.ex.reader.GrammarTokenizer
# tokenizer = uep.ex.reader.SimpleTokenizer
model_encoding=utf-8
lemmatizer=localhost:3456
# whether to cache parsed model and load it serialized next time
use_cache = 0
# html/xml parsing
html_create_text_nodes=1
html_skip_tags = script noscript style input option label textarea
# dir for temp docs
tmp_doc_dir = ./temp
# numeric value reading
number_group_seps= ,
number_float_seps=.

# Trainer
context_left_maxlen=2
context_right_maxlen=2
# dump_samples: 1 .. a weka arff file will be created for each document processed
#               2 .. a single weka arff file will be created for the whole collection
#               3 .. an "attribute=value" file will be created
dump_samples = 2
# deprecated, use task mode instead. mode of running classifiers: test or train; for train the processed collection must be labeled
# classifier_mode=test
# class_mode controls how training samples are generated and their classes set.
# class_mode = 0 to use all samples, all samples are classified as bg except for exact attribute values
# class_mode = 1 to use all samples, classify non-overlapping samples as bg, exact attribute values as attribute name, 
                 overlapping as attname_(contains|contained|cleft|cright|prefix|suffix|pleft|pright)
class_mode = 0

# Evaluator
eval_instances = 1
eval_instances_verbose = 0
eval_show_micro = 0

# Fetcher
save=1
replace_urls=1
keep_orig_urls=1
# keep_orig_urls keeps original urls as src_orig attributes
# mimetype_file=./mimetypes.txt # uncomment to override jar resource
fetch_frames=1
fetch_images=1
fetch_css=1
fetch_scripts=1
default_html_encoding=iso-8859-1
default_xml_encoding=utf-8

# MorphoClient
morphology_path=./src/tools/fm

# uep.ex.features.TagTypeF
# tag_type_file=./tagtypes.txt # uncomment to override jar resource

# other
# live_script=./livescript.js # uncomment to override jar resource

# Fetcher, Downloader
download_dir = ../web/download
# field used as page url in training object file
url_field = url_abs

# DbLoader, Downloader
db_driver = com.mysql.jdbc.Driver
data_encoding = utf-8

# Downloader
randomize=0
pause=5
download_prefix=med
resource_table=RESOURCE
link_table=REFS

# centrum: monitor descriptions
# db_connection=jdbc:mysql://localhost/centrum?user=ex&password=XXX&useUnicode=false&characterEncoding=iso-8859-2
# object_table=PRODUCT
#
# monitor images:
# link_table=IMG_REFS
# resource_table=IMG_RESOURCE
#
# medieq:
db_connection=jdbc:mysql://localhost/medcz1?user=root&password=XXX&useUnicode=true&characterEncoding=utf-8
# object_tables=OBJECT, PAGE, CONTACT, ARTICLE, CONSULT, AD, SEAL (multiple classes inheriting from OBJECT table - TBD)
object_table=PAGE

# misc
sentences = 1
dat_discard_oot_annots = 1
